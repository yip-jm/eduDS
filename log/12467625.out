Starting job on gcn147.local.snellius.surf.nl at Wed Jun 18 05:52:08 CEST 2025
Total CPUs allocated: 16
Number of CPUs allocated by Slurm=8
[INFO] ROOT_DIR set to /gpfs/home5/jye/dse
Using python: /gpfs/home5/jye/.venv/bin/python
Looking in links: https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
Requirement already satisfied: paddlepaddle-gpu==2.6.0 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (2.6.0)
Requirement already satisfied: httpx in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (0.28.1)
Requirement already satisfied: numpy>=1.13 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (2.2.5)
Requirement already satisfied: Pillow in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (11.2.1)
Requirement already satisfied: decorator in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (5.2.1)
Requirement already satisfied: astor in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (0.8.1)
Requirement already satisfied: opt-einsum==3.3.0 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (3.3.0)
Requirement already satisfied: protobuf>=3.20.2 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (3.20.3)
Requirement already satisfied: anyio in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (4.9.0)
Requirement already satisfied: certifi in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (2025.4.26)
Requirement already satisfied: httpcore==1.* in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (1.0.9)
Requirement already satisfied: idna in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (3.10)
Requirement already satisfied: h11>=0.16 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->paddlepaddle-gpu==2.6.0) (0.16.0)
Requirement already satisfied: sniffio>=1.1 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from anyio->httpx->paddlepaddle-gpu==2.6.0) (1.3.1)
Requirement already satisfied: typing_extensions>=4.5 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from anyio->httpx->paddlepaddle-gpu==2.6.0) (4.13.2)
apptainer version 1.4.1-1.el9
Wed Jun 18 05:52:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:26:00.0 Off |                    0 |
| N/A   33C    P0             69W /  700W |       1MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Checking available executables inside Singularity:
/sw/arch/RHEL8/EB_production/2023/software/cuDNN/8.9.2.26-CUDA-12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/nvvm/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/extras/CUPTI/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/OpenSSL/3/lib:/sw/arch/RHEL8/EB_production/2023/software/libffi/3.4.4-GCCcore-12.3.0/lib64:/sw/arch/RHEL8/EB_production/2023/software/XZ/5.4.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/SQLite/3.42.0-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/Tcl/8.6.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/libreadline/8.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/ncurses/6.4-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/bzip2/1.0.8-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/binutils/2.40-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/zlib/1.2.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/GCCcore/12.3.0/lib64
/usr/bin/ollama
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: deepseek-llm:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:52:15 | 200 |    4.860186ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:52:15 | 200 |    1.741481ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:52:15 | 200 |    1.636582ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:52:16 | 200 |       34.19µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:16 | 200 |  474.213317ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:52:16 | 200 |       30.81µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:17 | 200 |   46.359649ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:19 | 200 |  2.400901288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:52:19 | 200 |       25.41µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:20 | 200 |  413.597199ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:52:20 | 200 |       27.89µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:20 | 200 |   32.799727ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:22 | 200 |  2.050741552s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:52:22 | 200 |       36.96µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:23 | 200 |   417.57266ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:52:23 | 200 |       26.24µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:23 | 200 |   59.298524ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:26 | 200 |  2.500193303s |       127.0.0.1 | POST     "/api/generate"
Job completed at Wed Jun 18 05:52:40 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: gemma:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:52:40 | 200 |    1.846881ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:52:40 | 200 |    1.711162ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:52:40 | 200 |    1.631452ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:52:40 | 200 |       32.14µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:41 | 200 |  430.190607ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:52:41 | 200 |       38.98µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:41 | 200 |   49.057756ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:41 | 200 |   23.523553ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:52:41 | 200 |      32.559µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:42 | 200 |  450.068128ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:52:42 | 200 |       25.95µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:42 | 200 |    56.14594ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:42 | 200 |   30.327069ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:52:43 | 200 |       27.74µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:43 | 200 |  427.920217ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:52:43 | 200 |       34.99µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:43 | 200 |   54.934556ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:43 | 200 |   28.392269ms |       127.0.0.1 | POST     "/api/generate"
Job completed at Wed Jun 18 05:52:56 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: qwen2.5:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:52:56 | 200 |    1.799041ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:52:56 | 200 |    1.731081ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:52:56 | 200 |    1.662132ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:52:57 | 200 |       31.68µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:57 | 200 |  431.172506ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:52:58 | 200 |       25.01µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:58 | 200 |   50.548698ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:52:58 | 200 |   23.212804ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:52:58 | 200 |        28.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:58 | 200 |  424.475199ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:52:59 | 200 |       26.35µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:52:59 | 200 |   34.344519ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:02 | 200 |   3.16861085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:53:02 | 200 |        32.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:03 | 200 |  410.172021ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:53:03 | 200 |      25.979µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:03 | 200 |   55.259244ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:03 | 200 |   28.675496ms |       127.0.0.1 | POST     "/api/generate"
Job completed at Wed Jun 18 05:53:16 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: openchat:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:53:16 | 200 |    3.097174ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:53:16 | 200 |    1.619662ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:53:16 | 200 |    1.532153ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:53:16 | 200 |        33.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:17 | 200 |    434.2072ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:53:17 | 200 |       23.86µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:17 | 200 |   43.788592ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:17 | 200 |   23.290124ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:53:17 | 200 |       27.23µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:18 | 200 |  401.353645ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:53:18 | 200 |      26.819µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:18 | 200 |   14.287508ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:21 | 200 |  2.747529914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:53:21 | 200 |       31.87µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:22 | 200 |  417.110555ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:53:22 | 200 |       30.78µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:22 | 200 |   54.565878ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:22 | 200 |   28.393678ms |       127.0.0.1 | POST     "/api/generate"
Job completed at Wed Jun 18 05:53:35 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: llama3.1:8b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:53:35 | 200 |    1.701111ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:53:35 | 200 |    1.638222ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:53:35 | 200 |    1.574472ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:53:35 | 200 |       27.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:35 | 200 |  422.454639ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:53:36 | 200 |       27.15µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:36 | 200 |   45.730312ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:36 | 200 |   23.406013ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:53:36 | 200 |       28.73µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:37 | 200 |  439.104206ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:53:37 | 200 |        26.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:37 | 200 |   38.772286ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:40 | 200 |  3.230893999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:53:41 | 200 |       35.06µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:41 | 200 |  407.312465ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:53:41 | 200 |        36.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:53:41 | 200 |   55.193124ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:53:41 | 200 |   28.578217ms |       127.0.0.1 | POST     "/api/generate"

JOB STATISTICS
==============
Job ID: 12467625
Cluster: snellius
User/Group: jye/jye
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:01:12
CPU Efficiency: 4.21% of 00:28:32 core-walltime
Job Wall-clock time: 00:01:47
Memory Utilized: 3.62 GB
Memory Efficiency: 11.30% of 32.00 GB (32.00 GB/node)
