Starting job on gcn147.local.snellius.surf.nl at Wed Jun 18 05:54:05 CEST 2025
Total CPUs allocated: 16
Number of CPUs allocated by Slurm=8
[INFO] ROOT_DIR set to /gpfs/home5/jye/dse
Using python: /gpfs/home5/jye/.venv/bin/python
Looking in links: https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
Requirement already satisfied: paddlepaddle-gpu==2.6.0 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (2.6.0)
Requirement already satisfied: httpx in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (0.28.1)
Requirement already satisfied: numpy>=1.13 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (2.2.5)
Requirement already satisfied: Pillow in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (11.2.1)
Requirement already satisfied: decorator in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (5.2.1)
Requirement already satisfied: astor in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (0.8.1)
Requirement already satisfied: opt-einsum==3.3.0 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (3.3.0)
Requirement already satisfied: protobuf>=3.20.2 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from paddlepaddle-gpu==2.6.0) (3.20.3)
Requirement already satisfied: anyio in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (4.9.0)
Requirement already satisfied: certifi in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (2025.4.26)
Requirement already satisfied: httpcore==1.* in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (1.0.9)
Requirement already satisfied: idna in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpx->paddlepaddle-gpu==2.6.0) (3.10)
Requirement already satisfied: h11>=0.16 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->paddlepaddle-gpu==2.6.0) (0.16.0)
Requirement already satisfied: sniffio>=1.1 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from anyio->httpx->paddlepaddle-gpu==2.6.0) (1.3.1)
Requirement already satisfied: typing_extensions>=4.5 in /gpfs/home5/jye/.venv/lib/python3.11/site-packages (from anyio->httpx->paddlepaddle-gpu==2.6.0) (4.13.2)
apptainer version 1.4.1-1.el9
Wed Jun 18 05:54:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:26:00.0 Off |                    0 |
| N/A   33C    P0             68W /  700W |       1MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Checking available executables inside Singularity:
/sw/arch/RHEL8/EB_production/2023/software/cuDNN/8.9.2.26-CUDA-12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/nvvm/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/extras/CUPTI/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/OpenSSL/3/lib:/sw/arch/RHEL8/EB_production/2023/software/libffi/3.4.4-GCCcore-12.3.0/lib64:/sw/arch/RHEL8/EB_production/2023/software/XZ/5.4.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/SQLite/3.42.0-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/Tcl/8.6.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/libreadline/8.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/ncurses/6.4-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/bzip2/1.0.8-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/binutils/2.40-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/zlib/1.2.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/GCCcore/12.3.0/lib64
/usr/bin/ollama
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: deepseek-llm:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:54:13 | 200 |    4.924775ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:54:13 | 200 |    1.730521ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:54:13 | 200 |    1.516383ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:54:13 | 200 |       27.81µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:14 | 200 |  531.265118ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:54:14 | 200 |       26.34µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:14 | 200 |    47.69869ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:16 | 200 |   2.13947251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:54:17 | 200 |       40.21µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:17 | 200 |  397.227441ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:54:18 | 200 |       32.32µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:18 | 200 |   32.728676ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:20 | 200 |  2.055110745s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:54:20 | 200 |       28.13µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:20 | 200 |  404.440456ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:54:21 | 200 |       26.84µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:21 | 200 |   57.555051ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:24 | 200 |  2.740737926s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, deepseek-llm:7b, gemma:7b
Job completed at Wed Jun 18 05:54:37 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: gemma:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:54:37 | 200 |     1.93549ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:54:37 | 200 |    1.658562ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:54:37 | 200 |    1.644912ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:54:37 | 200 |       29.59µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:38 | 200 |  440.235516ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:54:38 | 200 |       28.53µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:38 | 200 |     47.6965ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:38 | 200 |   23.458032ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:54:38 | 200 |       29.05µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:39 | 200 |  406.159867ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:54:39 | 200 |       29.23µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:39 | 200 |   54.382036ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:39 | 200 |   28.912244ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:54:40 | 200 |       27.72µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:40 | 200 |  419.947908ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:54:40 | 200 |      26.329µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:40 | 200 |   54.375557ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:40 | 200 |   28.034029ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, gemma:7b, gemma:7b
Job completed at Wed Jun 18 05:54:54 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: qwen2.5:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:54:54 | 200 |     1.82717ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:54:54 | 200 |    1.713182ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:54:54 | 200 |    1.651852ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:54:55 | 200 |       37.98µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:55 | 200 |  462.699643ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:54:55 | 200 |       31.18µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:55 | 200 |   47.942309ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:54:56 | 200 |    23.76954ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:54:56 | 200 |       33.23µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:56 | 200 |  398.153747ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:54:57 | 200 |      28.709µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:54:57 | 200 |   34.291548ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:00 | 200 |  3.174803643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:00 | 200 |       28.63µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:01 | 200 |  417.408861ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:55:01 | 200 |       30.51µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:01 | 200 |   55.278042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:01 | 200 |   28.077229ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, qwen2.5:7b, gemma:7b
Job completed at Wed Jun 18 05:55:13 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: openchat:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:55:14 | 200 |     1.82382ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:55:14 | 200 |    1.710602ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:55:14 | 200 |    1.623502ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:55:14 | 200 |      27.879µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:14 | 200 |  397.835177ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:55:15 | 200 |       26.07µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:15 | 200 |   44.303648ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:15 | 200 |   23.356293ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:15 | 200 |       28.92µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:16 | 200 |  461.862225ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:55:16 | 200 |       29.23µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:16 | 200 |   15.549122ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:19 | 200 |  2.734546883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:19 | 200 |       29.88µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:19 | 200 |  417.159849ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:55:20 | 200 |      26.389µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:20 | 200 |   54.981674ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:20 | 200 |    27.94265ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, openchat:7b, gemma:7b
Job completed at Wed Jun 18 05:55:32 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: llama3.1:8b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:55:32 | 200 |    1.754802ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:55:32 | 200 |    1.570072ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:55:32 | 200 |    1.774031ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:55:33 | 200 |       29.12µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:33 | 200 |  404.694201ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:55:34 | 200 |       29.07µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:34 | 200 |   44.966645ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:34 | 200 |   23.573512ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:34 | 200 |       39.55µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:34 | 200 |  405.270518ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:55:35 | 200 |       32.95µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:35 | 200 |   38.519527ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:38 | 200 |  3.254161068s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:38 | 200 |       28.35µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:39 | 200 |  406.619832ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:55:39 | 200 |      29.089µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:39 | 200 |   55.074274ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:39 | 200 |   28.836016ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, llama3.1:8b, gemma:7b
Job completed at Wed Jun 18 05:55:55 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: olmo2:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:55:55 | 200 |     1.90605ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:55:55 | 200 |    1.806211ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:55:55 | 200 |    1.603182ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:55:55 | 200 |        25.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:55 | 200 |  414.506152ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:55:56 | 200 |       26.39µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:56 | 200 |   44.450177ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:55:56 | 200 |   22.979975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:55:56 | 200 |       32.94µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:57 | 200 |   446.88838ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:55:57 | 200 |       27.24µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:55:57 | 200 |   25.095174ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:56:01 | 200 |  4.056226978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:56:01 | 200 |        30.1µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:02 | 200 |  414.072755ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:56:02 | 200 |       38.05µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:02 | 200 |    53.85084ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:56:02 | 200 |   28.128789ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, olmo2:7b, gemma:7b
Job completed at Wed Jun 18 05:56:15 CEST 2025
=================================================================
Starting Experiment with:
  Embedding: BAAI/bge-large-en-v1.5
  LLM Model: phi4:14b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/18 - 05:56:15 | 200 |    3.134015ms |             ::1 | GET      "/api/tags"
Ollama for RAG server is ready!
[GIN] 2025/06/18 - 05:56:15 | 200 |     2.17306ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/18 - 05:56:15 | 200 |    1.578272ms |             ::1 | GET      "/api/tags"
Ollama for VL-LLM server is ready!
[GIN] 2025/06/18 - 05:56:16 | 200 |      28.799µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:16 | 200 |  442.197622ms |       127.0.0.1 | POST     "/api/pull"
Ollama model -- qwen2.5vl:7b is downloaded!
[GIN] 2025/06/18 - 05:56:16 | 200 |       26.58µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:16 | 200 |    43.87985ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:56:16 | 200 |   23.501722ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:56:17 | 200 |      31.719µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:17 | 200 |  422.583641ms |       127.0.0.1 | POST     "/api/pull"
Ollama RAG model is downloaded!
[GIN] 2025/06/18 - 05:56:18 | 200 |       33.17µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:18 | 200 |   24.025269ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:56:21 | 200 |  3.601827233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/18 - 05:56:21 | 200 |        32.9µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:22 | 200 |  407.536566ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/18 - 05:56:22 | 200 |       28.61µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/18 - 05:56:22 | 200 |   54.879625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/18 - 05:56:22 | 200 |    30.05789ms |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: BAAI/bge-large-en-v1.5, phi4:14b, gemma:7b
Job completed at Wed Jun 18 05:56:35 CEST 2025
All jobs completed at Wed Jun 18 05:56:35 CEST 2025

JOB STATISTICS
==============
Job ID: 12467626
Cluster: snellius
User/Group: jye/jye
State: RUNNING
Nodes: 1
Cores per node: 16
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:42:40 core-walltime
Job Wall-clock time: 00:02:40
Memory Utilized: 0.00 MB
Memory Efficiency: 0.00% of 32.00 GB (32.00 GB/node)
WARNING: Efficiency statistics can only be obtained after the job has ended as seff tool is based on the accounting database data.
