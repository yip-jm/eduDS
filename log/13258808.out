Starting job on gcn137.local.snellius.surf.nl at Wed Jul 23 03:42:41 CEST 2025
Total CPUs allocated: 16
Number of CPUs allocated by Slurm=8
[INFO] ROOT_DIR set to /gpfs/home5/jye/dse
Using python: /gpfs/home5/jye/.venv/bin/python
apptainer version 1.4.1-1.el9
Wed Jul 23 03:42:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:26:00.0 Off |                    0 |
| N/A   35C    P0            102W /  700W |       1MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Checking available executables inside Singularity:
/sw/arch/RHEL8/EB_production/2023/software/cuDNN/8.9.2.26-CUDA-12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/nvvm/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/extras/CUPTI/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/OpenSSL/3/lib:/sw/arch/RHEL8/EB_production/2023/software/libffi/3.4.4-GCCcore-12.3.0/lib64:/sw/arch/RHEL8/EB_production/2023/software/XZ/5.4.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/SQLite/3.42.0-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/Tcl/8.6.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/libreadline/8.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/ncurses/6.4-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/bzip2/1.0.8-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/binutils/2.40-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/zlib/1.2.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/GCCcore/12.3.0/lib64
/usr/bin/ollama
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:42:49 | 200 |    8.504281ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:42:49 | 200 |       31.85µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:42:50 | 200 |  460.724525ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:42:50 | 200 |       29.65µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:42:50 | 200 |   36.209915ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:42:55 | 200 |  4.807354994s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:43:17 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:43:23 | 200 |    4.618428ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:43:23 | 200 |      39.429µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:43:23 | 200 |  539.204644ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:43:24 | 200 |       30.14µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:43:24 | 200 |   61.728617ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:43:30 | 200 |  6.012226265s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:43:37 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:43:42 | 200 |    7.203159ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:43:42 | 200 |        26.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:43:43 | 200 |    487.2056ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:43:43 | 200 |       30.97µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:43:43 | 200 |   40.373207ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:43:48 | 200 |  5.081918427s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:43:55 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:44:00 | 200 |    7.437507ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:44:01 | 200 |       28.58µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:01 | 200 |  463.884433ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:44:01 | 200 |      27.399µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:01 | 200 |   18.064483ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:44:06 | 200 |   4.34525458s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:44:13 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:44:18 | 200 |    7.233519ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:44:18 | 200 |      32.439µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:19 | 200 |  473.545825ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:44:19 | 200 |      28.959µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:19 | 200 |   43.161456ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:44:25 | 200 |  5.408402854s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:44:32 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:44:37 | 200 |    8.037923ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:44:37 | 200 |       36.43µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:37 | 200 |  463.873853ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:44:38 | 200 |       31.14µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:38 | 200 |   27.737555ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:44:43 | 200 |   4.76824655s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:44:49 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:44:54 | 200 |    8.032824ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:44:55 | 200 |       24.97µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:55 | 200 |  478.929927ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:44:56 | 200 |       30.24µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:44:56 | 200 |   28.938947ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:45:05 | 200 |  9.777117249s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Wed Jul 23 03:45:13 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:45:18 | 200 |    7.271788ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:45:19 | 200 |      33.599µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:19 | 200 |  513.028748ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:45:20 | 200 |       27.55µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:20 | 200 |   33.561635ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:45:22 | 200 |  2.552920134s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:45:29 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:45:34 | 200 |    7.577197ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:45:34 | 200 |          32µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:35 | 200 |  498.205292ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:45:35 | 200 |        27.9µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:35 | 200 |   60.711004ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:45:38 | 200 |  2.991644624s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:45:45 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:45:50 | 200 |    8.238732ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:45:50 | 200 |        31.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:51 | 200 |  495.095874ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:45:51 | 200 |       27.59µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:45:51 | 200 |   37.175328ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:45:54 | 200 |  2.590405552s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:46:01 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:46:06 | 200 |    7.757456ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:46:06 | 200 |        30.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:07 | 200 |   504.10825ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:46:07 | 200 |       32.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:07 | 200 |   14.954055ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:46:09 | 200 |  2.340469387s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:46:17 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:46:22 | 200 |    6.946171ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:46:22 | 200 |       88.52µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:23 | 200 |  452.758162ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:46:23 | 200 |      31.669µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:23 | 200 |   41.553598ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:46:28 | 200 |  5.400176362s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:46:35 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:46:40 | 200 |    4.940425ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:46:41 | 200 |      33.369µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:42 | 200 |  792.531261ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:46:42 | 200 |       27.03µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:46:42 | 200 |   25.576269ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:46:47 | 200 |  5.515522675s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:46:55 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: qwen2.5:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:47:00 | 200 |    4.962455ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:47:01 | 200 |       26.73µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:01 | 200 |  464.633439ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:47:01 | 200 |       28.33µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:01 | 200 |   25.335961ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:47:05 | 200 |  3.252086138s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: qwen2.5:7b
Job completed at Wed Jul 23 03:47:12 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:47:17 | 200 |    4.445099ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:47:17 | 200 |      33.059µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:17 | 200 |   475.93907ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:47:18 | 200 |      32.149µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:18 | 200 |   34.534647ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:47:23 | 200 |   4.80596689s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:47:30 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:47:35 | 200 |    4.576558ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:47:35 | 200 |      61.399µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:36 | 200 |  486.933953ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:47:36 | 200 |       27.24µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:36 | 200 |   58.527348ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:47:42 | 200 |  6.011574653s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:47:49 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:47:54 | 200 |    5.456132ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:47:55 | 200 |      35.579µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:55 | 200 |  463.741326ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:47:55 | 200 |       26.85µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:47:55 | 200 |   34.887684ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:48:01 | 200 |  5.482757995s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:48:08 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:48:13 | 200 |    4.574798ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:48:14 | 200 |       32.63µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:14 | 200 |  460.640927ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:48:14 | 200 |       26.58µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:14 | 200 |   15.620751ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:48:19 | 200 |  4.654563306s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:48:26 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:48:31 | 200 |    4.933105ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:48:32 | 200 |      31.019µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:32 | 200 |  503.284768ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:48:32 | 200 |       32.23µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:32 | 200 |   42.804359ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:48:37 | 200 |  4.160253255s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:48:44 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:48:49 | 200 |    4.907215ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:48:50 | 200 |       31.46µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:50 | 200 |  497.246809ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:48:50 | 200 |       27.42µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:48:51 | 200 |   27.694185ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:48:54 | 200 |  3.512726083s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:49:01 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: openchat:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...
[GIN] 2025/07/23 - 03:49:06 | 200 |    5.035885ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/07/23 - 03:49:07 | 200 |        36.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:49:07 | 200 |  516.552543ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/07/23 - 03:49:07 | 200 |       27.71µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/23 - 03:49:07 | 200 |   24.465327ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/23 - 03:49:16 | 200 |  8.781255417s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: openchat:7b
Job completed at Wed Jul 23 03:49:24 CEST 2025
All jobs completed at Wed Jul 23 03:49:24 CEST 2025

JOB STATISTICS
==============
Job ID: 13258808
Cluster: snellius
User/Group: jye/jye
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:05:34
CPU Efficiency: 5.07% of 01:49:52 core-walltime
Job Wall-clock time: 00:06:52
Memory Utilized: 3.52 GB
Memory Efficiency: 11.01% of 32.00 GB (32.00 GB/node)
