Starting job on gcn122.local.snellius.surf.nl at Thu Jun 19 00:21:36 CEST 2025
Total CPUs allocated: 16
Number of CPUs allocated by Slurm=8
[INFO] ROOT_DIR set to /gpfs/home5/jye/dse
Using python: /gpfs/home5/jye/.venv/bin/python
apptainer version 1.4.1-1.el9
Thu Jun 19 00:21:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:26:00.0 Off |                    0 |
| N/A   32C    P0             68W /  700W |       1MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Checking available executables inside Singularity:
/sw/arch/RHEL8/EB_production/2023/software/cuDNN/8.9.2.26-CUDA-12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/nvvm/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/extras/CUPTI/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/OpenSSL/3/lib:/sw/arch/RHEL8/EB_production/2023/software/libffi/3.4.4-GCCcore-12.3.0/lib64:/sw/arch/RHEL8/EB_production/2023/software/XZ/5.4.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/SQLite/3.42.0-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/Tcl/8.6.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/libreadline/8.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/ncurses/6.4-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/bzip2/1.0.8-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/binutils/2.40-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/zlib/1.2.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/GCCcore/12.3.0/lib64
/usr/bin/ollama
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:21:44 | 200 |    6.633917ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:21:44 | 200 |       31.32µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:21:44 | 200 |  499.883858ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:21:45 | 200 |       27.68µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:21:45 | 200 |   36.875652ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:21:49 | 200 |  4.542228682s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:22:03 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:22:08 | 200 |    4.627364ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:22:09 | 200 |       35.94µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:09 | 200 |  493.114442ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:22:09 | 200 |       31.22µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:10 | 200 |   60.751389ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:22:15 | 200 |  5.733094822s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:22:22 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:22:27 | 200 |    5.943549ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:22:27 | 200 |       43.85µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:28 | 200 |  530.781498ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:22:28 | 200 |       28.62µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:28 | 200 |   38.227047ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:22:33 | 200 |  5.081254454s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:22:40 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:22:45 | 200 |    4.465934ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:22:45 | 200 |      31.469µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:46 | 200 |  466.721312ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:22:46 | 200 |       31.18µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:22:46 | 200 |   20.612988ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:22:51 | 200 |  4.380734238s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:22:58 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:23:03 | 200 |    4.939733ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:23:03 | 200 |       32.37µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:04 | 200 |   490.17531ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:23:04 | 200 |       32.35µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:04 | 200 |    42.98688ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:23:09 | 200 |   5.40650531s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:23:16 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:23:21 | 200 |    4.847483ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:23:22 | 200 |       34.79µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:22 | 200 |  547.588799ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:23:23 | 200 |      29.079µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:23 | 200 |   26.992946ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:23:27 | 200 |  4.732205148s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:23:34 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:23:39 | 200 |    7.746163ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:23:39 | 200 |       38.52µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:40 | 200 |  470.786329ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:23:40 | 200 |       34.74µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:23:40 | 200 |   27.750913ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:23:50 | 200 |  9.755408941s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:23:57 CEST 2025
All jobs completed at Thu Jun 19 00:23:57 CEST 2025

JOB STATISTICS
==============
Job ID: 12484217
Cluster: snellius
User/Group: jye/jye
State: RUNNING
Nodes: 1
Cores per node: 16
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:40:16 core-walltime
Job Wall-clock time: 00:02:31
Memory Utilized: 0.00 MB
Memory Efficiency: 0.00% of 32.00 GB (32.00 GB/node)
WARNING: Efficiency statistics can only be obtained after the job has ended as seff tool is based on the accounting database data.
