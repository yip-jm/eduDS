Starting job on gcn146.local.snellius.surf.nl at Thu Jun 19 00:12:35 CEST 2025
Total CPUs allocated: 16
Number of CPUs allocated by Slurm=8
[INFO] ROOT_DIR set to /gpfs/home5/jye/dse
Using python: /gpfs/home5/jye/.venv/bin/python
apptainer version 1.4.1-1.el9
Thu Jun 19 00:12:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100                    On  |   00000000:26:00.0 Off |                    0 |
| N/A   32C    P0             66W /  700W |       1MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Checking available executables inside Singularity:
/sw/arch/RHEL8/EB_production/2023/software/cuDNN/8.9.2.26-CUDA-12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/nvvm/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/extras/CUPTI/lib64:/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib:/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/OpenSSL/3/lib:/sw/arch/RHEL8/EB_production/2023/software/libffi/3.4.4-GCCcore-12.3.0/lib64:/sw/arch/RHEL8/EB_production/2023/software/XZ/5.4.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/SQLite/3.42.0-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/Tcl/8.6.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/libreadline/8.2-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/ncurses/6.4-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/bzip2/1.0.8-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/binutils/2.40-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/zlib/1.2.13-GCCcore-12.3.0/lib:/sw/arch/RHEL8/EB_production/2023/software/GCCcore/12.3.0/lib64
/usr/bin/ollama
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:12:43 | 200 |    6.195622ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:12:43 | 200 |        36.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:12:44 | 200 |   497.09027ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:12:44 | 200 |       28.41µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:12:44 | 200 |   35.949589ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:12:49 | 200 |  4.559677313s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:13:05 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:13:10 | 200 |     4.91323ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:13:11 | 200 |       28.39µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:11 | 200 |   536.26442ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:13:11 | 200 |       31.43µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:11 | 200 |    58.69625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:13:17 | 200 |  5.487267131s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:13:24 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:13:29 | 200 |    4.421493ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:13:29 | 200 |       33.27µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:29 | 200 |  510.741458ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:13:30 | 200 |       24.86µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:30 | 200 |   41.839804ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:13:35 | 200 |  4.844354718s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:13:41 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:13:46 | 200 |    5.423486ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:13:47 | 200 |       30.29µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:47 | 200 |  514.641545ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:13:47 | 200 |       27.46µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:13:48 | 200 |   17.801561ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:13:52 | 200 |   4.15234522s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:13:59 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:14:04 | 200 |    5.210468ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:14:04 | 200 |        37.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:05 | 200 |  483.792034ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:14:05 | 200 |       28.51µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:05 | 200 |    44.00284ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:14:10 | 200 |  5.159772524s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:14:17 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:14:22 | 200 |     4.94447ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:14:22 | 200 |        40.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:23 | 200 |  475.383285ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:14:23 | 200 |       27.46µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:23 | 200 |   27.178073ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:14:28 | 200 |  4.753078128s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:14:35 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: deepseek-llm:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:14:40 | 200 |     4.87731ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:14:40 | 200 |       27.49µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:40 | 200 |   481.19707ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:14:41 | 200 |       26.62µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:14:41 | 200 |   29.791017ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:14:50 | 200 |  9.514472864s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: deepseek-llm:7b
Job completed at Thu Jun 19 00:14:58 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: deepseek-llm:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:15:03 | 200 |     5.01753ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:15:03 | 200 |       31.87µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:03 | 200 |  508.748961ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:15:04 | 200 |       32.85µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:04 | 200 |   31.358328ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:15:06 | 200 |  2.556368216s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:15:13 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: gemma:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:15:18 | 200 |    4.614782ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:15:18 | 200 |      35.679µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:19 | 200 |  483.786144ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:15:19 | 200 |        27.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:19 | 200 |   59.042868ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:15:22 | 200 |  2.985791683s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:15:29 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: qwen2.5:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:15:34 | 200 |    4.425552ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:15:35 | 200 |       36.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:35 | 200 |  559.920677ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:15:36 | 200 |       31.06µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:36 | 200 |   38.164746ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:15:38 | 200 |  2.585669526s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:15:45 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: openchat:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:15:50 | 200 |    4.542382ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:15:50 | 200 |      38.889µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:51 | 200 |  475.768083ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:15:51 | 200 |       28.34µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:15:51 | 200 |   16.214811ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:15:53 | 200 |  2.140295858s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:16:00 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: llama3.1:8b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:16:05 | 200 |    5.760485ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:16:05 | 200 |       28.33µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:16:06 | 200 |  526.729841ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:16:06 | 200 |       32.37µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:16:06 | 200 |   40.022655ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:16:11 | 200 |  5.171200874s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:16:18 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: olmo2:7b
=================================================================
Starting Ollama server...
[GIN] 2025/06/19 - 00:16:23 | 200 |    5.702905ms |             ::1 | GET      "/api/tags"
Ollama for SYLLM server is ready!
[GIN] 2025/06/19 - 00:16:24 | 200 |          39µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:16:24 | 200 |  501.369336ms |       127.0.0.1 | POST     "/api/pull"
Ollama SYLLM model is downloaded!
[GIN] 2025/06/19 - 00:16:24 | 200 |       24.92µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/19 - 00:16:24 | 200 |   27.015835ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/06/19 - 00:16:30 | 200 |  5.263319309s |       127.0.0.1 | POST     "/api/generate"
Running Python script with models: gemma:7b
Job completed at Thu Jun 19 00:16:37 CEST 2025
=================================================================
Starting Experiment with:
  RAG Model: gemma:7b
  Story Model: phi4:14b
=================================================================
Starting Ollama server...

JOB STATISTICS
==============
Job ID: 12484157
Cluster: snellius
User/Group: jye/jye
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:03:28
CPU Efficiency: 5.18% of 01:06:56 core-walltime
Job Wall-clock time: 00:04:11
Memory Utilized: 1.23 GB
Memory Efficiency: 3.85% of 32.00 GB (32.00 GB/node)
